# -*- coding: utf-8 -*-
"""
å¢å¼ºç›¸ä¼¼åº¦æ£€æµ‹æ¨¡å—

æ”¯æŒæ—‹è½¬å’Œåˆ†è¾¨ç‡å˜åŒ–çš„å›¾ç‰‡ç›¸ä¼¼åº¦æ£€æµ‹
ç»“åˆå¤šç§å“ˆå¸Œç®—æ³•å’Œå¤šå°ºåº¦åˆ†ææé«˜æ£€æµ‹ç²¾åº¦
"""

import os
import tempfile
from typing import List, Dict, Tuple, Optional
from PIL import Image
import numpy as np
from .hash_utils import calculate_dhash, calculate_phash, calculate_ahash, hamming_distance


class EnhancedSimilarityDetector:
    """
    å¢å¼ºç›¸ä¼¼åº¦æ£€æµ‹å™¨
    æ”¯æŒæ—‹è½¬ã€ç¼©æ”¾ã€åˆ†è¾¨ç‡å˜åŒ–çš„å›¾ç‰‡ç›¸ä¼¼åº¦æ£€æµ‹
    """
    
    def __init__(self, 
                 angles: List[int] = None,
                 scales: List[float] = None,
                 hash_sizes: List[int] = None):
        """
        åˆå§‹åŒ–å¢å¼ºç›¸ä¼¼åº¦æ£€æµ‹å™¨
        
        Args:
            angles: è¦æ£€æµ‹çš„æ—‹è½¬è§’åº¦åˆ—è¡¨ï¼Œé»˜è®¤ä¸º[0, 90, 180, 270]
            scales: è¦æ£€æµ‹çš„ç¼©æ”¾æ¯”ä¾‹åˆ—è¡¨ï¼Œé»˜è®¤ä¸º[0.5, 0.75, 1.0, 1.25, 1.5]
            hash_sizes: å“ˆå¸Œå¤§å°åˆ—è¡¨ï¼Œç”¨äºå¤šå°ºåº¦åˆ†æï¼Œé»˜è®¤ä¸º[8, 16]
        """
        # ä¼˜åŒ–é»˜è®¤å‚æ•°ä»¥æé«˜æ€§èƒ½
        self.angles = angles or [0, 90, 180, 270]  # ä¿æŒæ—‹è½¬æ£€æµ‹
        self.scales = scales or [0.75, 1.0, 1.25]  # å‡å°‘ç¼©æ”¾æ¯”ä¾‹
        self.hash_sizes = hash_sizes or [8]  # åªä½¿ç”¨ä¸€ä¸ªå“ˆå¸Œå¤§å°
    
    def calculate_multi_scale_hashes(self, image_path: str) -> List[Dict]:
        """
        è®¡ç®—å›¾ç‰‡åœ¨å¤šä¸ªè§’åº¦å’Œå°ºåº¦ä¸‹çš„å“ˆå¸Œå€¼
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            
        Returns:
            åŒ…å«å¤šä¸ªè§’åº¦å’Œå°ºåº¦å“ˆå¸Œå€¼çš„åˆ—è¡¨
        """
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        
        all_hashes = []
        
        try:
            img = Image.open(image_path)
            original_size = img.size
            
            for angle in self.angles:
                # æ—‹è½¬å›¾ç‰‡
                if angle == 0:
                    rotated_img = img
                else:
                    rotated_img = img.rotate(angle, expand=True, fillcolor='white')
                
                for scale in self.scales:
                    # ç¼©æ”¾å›¾ç‰‡
                    if scale == 1.0:
                        scaled_img = rotated_img
                    else:
                        new_size = (int(rotated_img.size[0] * scale), 
                                   int(rotated_img.size[1] * scale))
                        if new_size[0] > 0 and new_size[1] > 0:
                            scaled_img = rotated_img.resize(new_size, Image.LANCZOS)
                        else:
                            continue
                    
                    # ä¸ºæ¯ä¸ªå“ˆå¸Œå¤§å°è®¡ç®—å“ˆå¸Œå€¼ï¼ˆä¼˜åŒ–ï¼šç›´æ¥ä»PILå›¾åƒè®¡ç®—ï¼‰
                    for hash_size in self.hash_sizes:
                        try:
                            # ç›´æ¥ä»PILå›¾åƒè®¡ç®—å“ˆå¸Œå€¼ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶
                            dhash = self._calculate_dhash_from_image(scaled_img, hash_size)
                            phash = self._calculate_phash_from_image(scaled_img, hash_size)
                            ahash = self._calculate_ahash_from_image(scaled_img, hash_size)
                            
                            # è®¡ç®—é¢å¤–çš„ç‰¹å¾
                            features = self._extract_additional_features(scaled_img)
                            
                            all_hashes.append({
                                'angle': angle,
                                'scale': scale,
                                'hash_size': hash_size,
                                'dhash': dhash,
                                'phash': phash,
                                'ahash': ahash,
                                'features': features,
                                'image_size': scaled_img.size
                            })
                        except Exception as e:
                            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªç»„åˆ
                            continue
        
        except Exception as e:
            raise ValueError(f"è®¡ç®—å¤šå°ºåº¦å“ˆå¸Œæ—¶å‡ºé”™: {image_path}, é”™è¯¯: {str(e)}")
        
        return all_hashes
    
    def _calculate_dhash_from_image(self, image: Image.Image, hash_size: int) -> str:
        """ç›´æ¥ä»PILå›¾åƒè®¡ç®—æŒ‡å®šå¤§å°çš„dHash"""
        try:
            gray_img = image.convert('L')
            resized_img = gray_img.resize((hash_size + 1, hash_size), Image.LANCZOS)
            pixels = np.array(resized_img)
            diff = pixels[:, 1:] > pixels[:, :-1]
            binary_hash = ''.join('1' if d else '0' for d in diff.flatten())
            hex_hash = ''
            for i in range(0, len(binary_hash), 4):
                hex_hash += hex(int(binary_hash[i:i+4], 2))[2:]
            return hex_hash
        except Exception:
            return "error"
    
    def _calculate_phash_from_image(self, image: Image.Image, hash_size: int) -> str:
        """ç›´æ¥ä»PILå›¾åƒè®¡ç®—æŒ‡å®šå¤§å°çš„pHash"""
        try:
            gray_img = image.convert('L')
            resized_img = gray_img.resize((hash_size * 4, hash_size * 4), Image.LANCZOS)
            pixels = np.array(resized_img, dtype=np.float32)
            
            # ç®€åŒ–çš„DCTè®¡ç®—
            dct = np.zeros((hash_size, hash_size))
            for i in range(hash_size):
                for j in range(hash_size):
                    dct[i, j] = np.sum(pixels * 
                                     np.cos(np.pi * i * np.arange(hash_size * 4) / (hash_size * 4)) *
                                     np.cos(np.pi * j * np.arange(hash_size * 4).reshape(-1, 1) / (hash_size * 4)))
            
            dct_flat = dct.flatten()[1:]
            median = np.median(dct_flat)
            diff = dct_flat > median
            binary_hash = ''.join('1' if d else '0' for d in diff)
            hex_hash = ''
            for i in range(0, len(binary_hash), 4):
                if i + 4 <= len(binary_hash):
                    hex_hash += hex(int(binary_hash[i:i+4], 2))[2:]
            return hex_hash
        except Exception:
            return "error"
    
    def _calculate_ahash_from_image(self, image: Image.Image, hash_size: int) -> str:
        """ç›´æ¥ä»PILå›¾åƒè®¡ç®—æŒ‡å®šå¤§å°çš„aHash"""
        try:
            gray_img = image.convert('L')
            resized_img = gray_img.resize((hash_size, hash_size), Image.LANCZOS)
            pixels = np.array(resized_img)
            avg = np.mean(pixels)
            diff = pixels > avg
            binary_hash = ''.join('1' if d else '0' for d in diff.flatten())
            hex_hash = ''
            for i in range(0, len(binary_hash), 4):
                if i + 4 <= len(binary_hash):
                    hex_hash += hex(int(binary_hash[i:i+4], 2))[2:]
            return hex_hash
        except Exception:
            return "error"
    
    def _calculate_dhash_with_size(self, image_path: str, hash_size: int) -> str:
        """è®¡ç®—æŒ‡å®šå¤§å°çš„dHash"""
        try:
            image = Image.open(image_path).convert('L')
            image = image.resize((hash_size + 1, hash_size), Image.LANCZOS)
            pixels = np.array(image)
            diff = pixels[:, 1:] > pixels[:, :-1]
            binary_hash = ''.join('1' if d else '0' for d in diff.flatten())
            hex_hash = ''
            for i in range(0, len(binary_hash), 4):
                hex_hash += hex(int(binary_hash[i:i+4], 2))[2:]
            return hex_hash
        except Exception:
            return "error"
    
    def _calculate_phash_with_size(self, image_path: str, hash_size: int) -> str:
        """è®¡ç®—æŒ‡å®šå¤§å°çš„pHash"""
        try:
            image = Image.open(image_path).convert('L')
            image = image.resize((hash_size * 4, hash_size * 4), Image.LANCZOS)
            pixels = np.array(image, dtype=np.float32)
            
            # ç®€åŒ–çš„DCTè®¡ç®—
            dct = np.zeros((hash_size, hash_size))
            for i in range(hash_size):
                for j in range(hash_size):
                    dct[i, j] = np.sum(pixels * 
                                     np.cos(np.pi * i * np.arange(hash_size * 4) / (hash_size * 4)) *
                                     np.cos(np.pi * j * np.arange(hash_size * 4).reshape(-1, 1) / (hash_size * 4)))
            
            dct_flat = dct.flatten()[1:]
            median = np.median(dct_flat)
            diff = dct_flat > median
            binary_hash = ''.join('1' if d else '0' for d in diff)
            hex_hash = ''
            for i in range(0, len(binary_hash), 4):
                if i + 4 <= len(binary_hash):
                    hex_hash += hex(int(binary_hash[i:i+4], 2))[2:]
            return hex_hash
        except Exception:
            return "error"
    
    def _calculate_ahash_with_size(self, image_path: str, hash_size: int) -> str:
        """è®¡ç®—æŒ‡å®šå¤§å°çš„aHash"""
        try:
            image = Image.open(image_path).convert('L')
            image = image.resize((hash_size, hash_size), Image.LANCZOS)
            pixels = np.array(image)
            avg = np.mean(pixels)
            diff = pixels > avg
            binary_hash = ''.join('1' if d else '0' for d in diff.flatten())
            hex_hash = ''
            for i in range(0, len(binary_hash), 4):
                if i + 4 <= len(binary_hash):
                    hex_hash += hex(int(binary_hash[i:i+4], 2))[2:]
            return hex_hash
        except Exception:
            return "error"
    
    def _extract_additional_features(self, image: Image.Image) -> Dict:
        """
        æå–é¢å¤–çš„å›¾ç‰‡ç‰¹å¾
        
        Args:
            image: PILå›¾ç‰‡å¯¹è±¡
            
        Returns:
            ç‰¹å¾å­—å…¸
        """
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray_img = image.convert('L')
            pixels = np.array(gray_img)
            
            features = {
                'aspect_ratio': image.size[0] / image.size[1] if image.size[1] > 0 else 1.0,
                'brightness': np.mean(pixels),
                'contrast': np.std(pixels),
                'entropy': self._calculate_entropy(pixels),
                'edge_density': self._calculate_edge_density(pixels)
            }
            
            return features
        except Exception:
            return {
                'aspect_ratio': 1.0,
                'brightness': 128.0,
                'contrast': 0.0,
                'entropy': 0.0,
                'edge_density': 0.0
            }
    
    def _calculate_entropy(self, pixels: np.ndarray) -> float:
        """è®¡ç®—å›¾ç‰‡ç†µå€¼"""
        try:
            hist, _ = np.histogram(pixels, bins=256, range=(0, 256))
            hist = hist / hist.sum()
            hist = hist[hist > 0]  # ç§»é™¤é›¶å€¼
            entropy = -np.sum(hist * np.log2(hist))
            return float(entropy)
        except Exception:
            return 0.0
    
    def _calculate_edge_density(self, pixels: np.ndarray) -> float:
        """è®¡ç®—è¾¹ç¼˜å¯†åº¦"""
        try:
            # ç®€å•çš„Sobelè¾¹ç¼˜æ£€æµ‹
            sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
            sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
            
            # è®¡ç®—æ¢¯åº¦
            grad_x = np.abs(np.convolve(pixels.flatten(), sobel_x.flatten(), mode='same'))
            grad_y = np.abs(np.convolve(pixels.flatten(), sobel_y.flatten(), mode='same'))
            
            edge_magnitude = np.sqrt(grad_x**2 + grad_y**2)
            edge_density = np.mean(edge_magnitude)
            
            return float(edge_density)
        except Exception:
            return 0.0
    
    def compare_enhanced_similarity(self, 
                                  image1_path: str, 
                                  image2_path: str,
                                  dhash_threshold: int = 12,
                                  phash_threshold: int = 4,
                                  ahash_threshold: int = 4,
                                  feature_weight: float = 0.3) -> Dict:
        """
        ä½¿ç”¨å¢å¼ºæ–¹æ³•æ¯”è¾ƒä¸¤å¼ å›¾ç‰‡çš„ç›¸ä¼¼åº¦
        
        Args:
            image1_path: ç¬¬ä¸€å¼ å›¾ç‰‡è·¯å¾„
            image2_path: ç¬¬äºŒå¼ å›¾ç‰‡è·¯å¾„
            dhash_threshold: dHashé˜ˆå€¼ï¼ˆæ”¾å®½ä»¥é€‚åº”åˆ†è¾¨ç‡å˜åŒ–ï¼‰
            phash_threshold: pHashé˜ˆå€¼ï¼ˆæ”¾å®½ä»¥é€‚åº”åˆ†è¾¨ç‡å˜åŒ–ï¼‰
            ahash_threshold: aHashé˜ˆå€¼ï¼ˆæ”¾å®½ä»¥é€‚åº”åˆ†è¾¨ç‡å˜åŒ–ï¼‰
            feature_weight: ç‰¹å¾ç›¸ä¼¼åº¦æƒé‡
            
        Returns:
            è¯¦ç»†çš„æ¯”è¾ƒç»“æœ
        """
        hashes1 = self.calculate_multi_scale_hashes(image1_path)
        hashes2 = self.calculate_multi_scale_hashes(image2_path)
        
        best_match = None
        min_distances = {
            'dhash': float('inf'),
            'phash': float('inf'),
            'ahash': float('inf')
        }
        
        all_matches = []
        
        # æ¯”è¾ƒæ‰€æœ‰ç»„åˆ
        for h1 in hashes1:
            for h2 in hashes2:
                if (h1['dhash'] == "error" or h2['dhash'] == "error" or
                    h1['phash'] == "error" or h2['phash'] == "error" or
                    h1['ahash'] == "error" or h2['ahash'] == "error"):
                    continue
                
                try:
                    # è®¡ç®—å“ˆå¸Œè·ç¦»
                    dhash_dist = hamming_distance(h1['dhash'], h2['dhash'])
                    phash_dist = hamming_distance(h1['phash'], h2['phash'])
                    ahash_dist = hamming_distance(h1['ahash'], h2['ahash'])
                    
                    # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
                    feature_similarity = self._calculate_feature_similarity(h1['features'], h2['features'])
                    
                    # ç»¼åˆè¯„åˆ†ï¼ˆå“ˆå¸Œè·ç¦»è¶Šå°è¶Šå¥½ï¼Œç‰¹å¾ç›¸ä¼¼åº¦è¶Šå¤§è¶Šå¥½ï¼‰
                    hash_score = (dhash_dist + phash_dist + ahash_dist) / 3.0
                    feature_score = (1.0 - feature_similarity) * 64  # è½¬æ¢ä¸ºè·ç¦»å½¢å¼
                    combined_score = hash_score * (1 - feature_weight) + feature_score * feature_weight
                    
                    match_info = {
                        'angle1': h1['angle'],
                        'angle2': h2['angle'],
                        'scale1': h1['scale'],
                        'scale2': h2['scale'],
                        'hash_size': h1['hash_size'],
                        'dhash_distance': dhash_dist,
                        'phash_distance': phash_dist,
                        'ahash_distance': ahash_dist,
                        'feature_similarity': feature_similarity,
                        'combined_score': combined_score,
                        'size1': h1['image_size'],
                        'size2': h2['image_size']
                    }
                    
                    all_matches.append(match_info)
                    
                    # æ›´æ–°æœ€å°è·ç¦»
                    if dhash_dist < min_distances['dhash']:
                        min_distances['dhash'] = dhash_dist
                    if phash_dist < min_distances['phash']:
                        min_distances['phash'] = phash_dist
                    if ahash_dist < min_distances['ahash']:
                        min_distances['ahash'] = ahash_dist
                    
                    # æ›´æ–°æœ€ä½³åŒ¹é…
                    if best_match is None or combined_score < best_match['combined_score']:
                        best_match = match_info
                
                except Exception:
                    continue
        
        # æ’åºæ‰€æœ‰åŒ¹é…ç»“æœ
        all_matches.sort(key=lambda x: x['combined_score'])
        
        # åˆ¤æ–­ç›¸ä¼¼åº¦
        dhash_similar = min_distances['dhash'] <= dhash_threshold
        phash_similar = min_distances['phash'] <= phash_threshold
        ahash_similar = min_distances['ahash'] <= ahash_threshold
        
        # ç‰¹å¾ç›¸ä¼¼åº¦åˆ¤æ–­
        feature_similar = False
        if best_match and best_match['feature_similarity'] > 0.7:
            feature_similar = True
        
        # ç»¼åˆåˆ¤æ–­ï¼ˆè‡³å°‘ä¸¤ç§ç®—æ³•ç›¸ä¼¼ï¼Œæˆ–è€…ç‰¹å¾é«˜åº¦ç›¸ä¼¼ï¼‰
        similar_count = sum([dhash_similar, phash_similar, ahash_similar])
        is_similar = similar_count >= 2 or (similar_count >= 1 and feature_similar)
        
        # ç¡®å®šæ£€æµ‹ç±»å‹
        detection_type = self._determine_detection_type(best_match)
        
        return {
            'is_similar': is_similar,
            'detection_type': detection_type,
            'min_distances': min_distances,
            'best_match': best_match,
            'top_matches': all_matches[:5],  # è¿”å›å‰5ä¸ªæœ€ä½³åŒ¹é…
            'similar_algorithms': {
                'dhash': dhash_similar,
                'phash': phash_similar,
                'ahash': ahash_similar,
                'features': feature_similar
            },
            'similar_count': similar_count,
            'confidence': self._calculate_confidence(best_match, min_distances)
        }
    
    def _calculate_feature_similarity(self, features1: Dict, features2: Dict) -> float:
        """
        è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
        
        Args:
            features1: ç¬¬ä¸€å¼ å›¾ç‰‡çš„ç‰¹å¾
            features2: ç¬¬äºŒå¼ å›¾ç‰‡çš„ç‰¹å¾
            
        Returns:
            ç‰¹å¾ç›¸ä¼¼åº¦ï¼ˆ0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸ä¼¼ï¼‰
        """
        try:
            # è®¡ç®—å„ä¸ªç‰¹å¾çš„ç›¸ä¼¼åº¦
            aspect_ratio_sim = 1.0 - min(abs(features1['aspect_ratio'] - features2['aspect_ratio']) / max(features1['aspect_ratio'], features2['aspect_ratio']), 1.0)
            brightness_sim = 1.0 - min(abs(features1['brightness'] - features2['brightness']) / 255.0, 1.0)
            contrast_sim = 1.0 - min(abs(features1['contrast'] - features2['contrast']) / 128.0, 1.0)
            entropy_sim = 1.0 - min(abs(features1['entropy'] - features2['entropy']) / 8.0, 1.0)
            edge_sim = 1.0 - min(abs(features1['edge_density'] - features2['edge_density']) / 100.0, 1.0)
            
            # åŠ æƒå¹³å‡
            weights = [0.15, 0.25, 0.25, 0.2, 0.15]  # å¯¹æ¯”åº¦å’Œäº®åº¦æƒé‡è¾ƒé«˜
            similarities = [aspect_ratio_sim, brightness_sim, contrast_sim, entropy_sim, edge_sim]
            
            weighted_similarity = sum(w * s for w, s in zip(weights, similarities))
            return weighted_similarity
        except Exception:
            return 0.0
    
    def _determine_detection_type(self, best_match: Optional[Dict]) -> str:
        """
        ç¡®å®šæ£€æµ‹ç±»å‹
        
        Args:
            best_match: æœ€ä½³åŒ¹é…ä¿¡æ¯
            
        Returns:
            æ£€æµ‹ç±»å‹æè¿°
        """
        if not best_match:
            return "æ— åŒ¹é…"
        
        angle_diff = abs(best_match['angle1'] - best_match['angle2'])
        if angle_diff > 180:
            angle_diff = 360 - angle_diff
        
        scale_diff = abs(best_match['scale1'] - best_match['scale2'])
        
        detection_parts = []
        
        if angle_diff > 0:
            detection_parts.append(f"æ—‹è½¬{angle_diff}Â°")
        
        if scale_diff > 0.1:
            scale_ratio = max(best_match['scale1'], best_match['scale2']) / min(best_match['scale1'], best_match['scale2'])
            detection_parts.append(f"ç¼©æ”¾{scale_ratio:.1f}x")
        
        if best_match['size1'] != best_match['size2']:
            detection_parts.append("åˆ†è¾¨ç‡å˜åŒ–")
        
        if not detection_parts:
            return "å®Œå…¨ç›¸åŒ"
        
        return "å¢å¼ºæ£€æµ‹(" + "+".join(detection_parts) + ")"
    
    def _calculate_confidence(self, best_match: Optional[Dict], min_distances: Dict) -> float:
        """
        è®¡ç®—æ£€æµ‹ç½®ä¿¡åº¦
        
        Args:
            best_match: æœ€ä½³åŒ¹é…ä¿¡æ¯
            min_distances: æœ€å°è·ç¦»
            
        Returns:
            ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´ï¼‰
        """
        if not best_match:
            return 0.0
        
        try:
            # åŸºäºå“ˆå¸Œè·ç¦»è®¡ç®—ç½®ä¿¡åº¦
            max_distance = 64  # æœ€å¤§å¯èƒ½çš„æ±‰æ˜è·ç¦»
            hash_confidence = 1.0 - (best_match['dhash_distance'] + best_match['phash_distance'] + best_match['ahash_distance']) / (3 * max_distance)
            
            # åŸºäºç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—ç½®ä¿¡åº¦
            feature_confidence = best_match['feature_similarity']
            
            # ç»¼åˆç½®ä¿¡åº¦
            combined_confidence = (hash_confidence * 0.7 + feature_confidence * 0.3)
            
            return max(0.0, min(1.0, combined_confidence))
        except Exception:
            return 0.0
    
    def _hamming_distance(self, hash1: str, hash2: str) -> int:
        """
        è®¡ç®—ä¸¤ä¸ªå“ˆå¸Œå€¼çš„æ±‰æ˜è·ç¦»
        
        Args:
            hash1: ç¬¬ä¸€ä¸ªå“ˆå¸Œå€¼
            hash2: ç¬¬äºŒä¸ªå“ˆå¸Œå€¼
            
        Returns:
            æ±‰æ˜è·ç¦»
        """
        if hash1 == "error" or hash2 == "error" or len(hash1) != len(hash2):
            return 999  # è¿”å›ä¸€ä¸ªå¾ˆå¤§çš„è·ç¦»å€¼
        
        try:
            return sum(c1 != c2 for c1, c2 in zip(hash1, hash2))
        except Exception:
            return 999
    
    def _calculate_feature_similarity_from_dict(self, features1: Dict, features2: Dict) -> float:
        """
        ä»ç‰¹å¾å­—å…¸è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
        
        Args:
            features1: ç¬¬ä¸€ä¸ªå›¾ç‰‡çš„ç‰¹å¾å­—å…¸
            features2: ç¬¬äºŒä¸ªå›¾ç‰‡çš„ç‰¹å¾å­—å…¸
            
        Returns:
            ç‰¹å¾ç›¸ä¼¼åº¦ (0-1)
        """
        try:
            # è®¡ç®—å„ä¸ªç‰¹å¾çš„ç›¸ä¼¼åº¦
            aspect_ratio_sim = 1.0 - abs(features1.get('aspect_ratio', 1.0) - features2.get('aspect_ratio', 1.0))
            brightness_sim = 1.0 - abs(features1.get('brightness', 0.5) - features2.get('brightness', 0.5))
            contrast_sim = 1.0 - abs(features1.get('contrast', 0.5) - features2.get('contrast', 0.5))
            entropy_sim = 1.0 - abs(features1.get('entropy', 5.0) - features2.get('entropy', 5.0)) / 10.0
            edge_density_sim = 1.0 - abs(features1.get('edge_density', 0.1) - features2.get('edge_density', 0.1))
            
            # åŠ æƒå¹³å‡
            similarity = (aspect_ratio_sim * 0.3 + brightness_sim * 0.2 + 
                         contrast_sim * 0.2 + entropy_sim * 0.15 + edge_density_sim * 0.15)
            
            return max(0.0, min(1.0, similarity))
        except Exception:
            return 0.8  # é»˜è®¤ç›¸ä¼¼åº¦
    
    def _fast_compare_with_precomputed_hashes(self, hashes1: List[Dict], hashes2: List[Dict],
                                            file1: str, file2: str,
                                            dhash_threshold: int = 12,
                                            phash_threshold: int = 4,
                                            ahash_threshold: int = 4,
                                            feature_weight: float = 0.3) -> Dict:
        """
        ä½¿ç”¨é¢„è®¡ç®—çš„å“ˆå¸Œå€¼è¿›è¡Œå¿«é€Ÿæ¯”è¾ƒ
        
        Args:
            hashes1: ç¬¬ä¸€ä¸ªå›¾ç‰‡çš„å¤šå°ºåº¦å“ˆå¸Œå€¼
            hashes2: ç¬¬äºŒä¸ªå›¾ç‰‡çš„å¤šå°ºåº¦å“ˆå¸Œå€¼
            file1: ç¬¬ä¸€ä¸ªå›¾ç‰‡è·¯å¾„
            file2: ç¬¬äºŒä¸ªå›¾ç‰‡è·¯å¾„
            dhash_threshold: dHashé˜ˆå€¼
            phash_threshold: pHashé˜ˆå€¼
            ahash_threshold: aHashé˜ˆå€¼
            feature_weight: ç‰¹å¾æƒé‡
            
        Returns:
            æ¯”è¾ƒç»“æœå­—å…¸
        """
        if not hashes1 or not hashes2:
            return None
        
        best_similarity = 0
        best_match = None
        best_rotation = 0
        best_scale = 1.0
        min_distances = {
            'dhash': float('inf'),
            'phash': float('inf'),
            'ahash': float('inf')
        }
        
        # å¿«é€Ÿæ¯”è¾ƒæ‰€æœ‰å“ˆå¸Œç»„åˆ
        for hash_data1 in hashes1:
            for hash_data2 in hashes2:
                # è®¡ç®—å“ˆå¸Œè·ç¦»
                dhash_dist = self._hamming_distance(hash_data1['dhash'], hash_data2['dhash'])
                phash_dist = self._hamming_distance(hash_data1['phash'], hash_data2['phash'])
                ahash_dist = self._hamming_distance(hash_data1['ahash'], hash_data2['ahash'])
                
                # æ›´æ–°æœ€å°è·ç¦»
                min_distances['dhash'] = min(min_distances['dhash'], dhash_dist)
                min_distances['phash'] = min(min_distances['phash'], phash_dist)
                min_distances['ahash'] = min(min_distances['ahash'], ahash_dist)
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é˜ˆå€¼
                hash_matches = 0
                if dhash_dist <= dhash_threshold:
                    hash_matches += 1
                if phash_dist <= phash_threshold:
                    hash_matches += 1
                if ahash_dist <= ahash_threshold:
                    hash_matches += 1
                
                # è‡³å°‘éœ€è¦2ä¸ªå“ˆå¸Œç®—æ³•åŒ¹é…
                if hash_matches >= 2:
                    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                    hash_similarity = 1.0 - (dhash_dist + phash_dist + ahash_dist) / (64 + 64 + 64)
                    
                    if hash_similarity > best_similarity:
                         best_similarity = hash_similarity
                         # ç®€åŒ–ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—
                         feature_sim = 0.8  # é»˜è®¤ç‰¹å¾ç›¸ä¼¼åº¦
                         if 'features' in hash_data1 and 'features' in hash_data2:
                             try:
                                 feature_sim = self._calculate_feature_similarity_from_dict(
                                     hash_data1['features'], hash_data2['features']
                                 )
                             except:
                                 feature_sim = 0.8
                         
                         best_match = {
                             'dhash_distance': dhash_dist,
                             'phash_distance': phash_dist,
                             'ahash_distance': ahash_dist,
                             'hash_similarity': hash_similarity,
                             'feature_similarity': feature_sim
                         }
                         best_rotation = abs(hash_data1.get('angle', 0) - hash_data2.get('angle', 0))
                         best_scale = max(hash_data1.get('scale', 1.0), hash_data2.get('scale', 1.0)) / min(hash_data1.get('scale', 1.0), hash_data2.get('scale', 1.0))
        
        if best_match is None:
            return {
                'is_similar': False,
                'confidence': 0.0,
                'detection_type': 'æ— åŒ¹é…',
                'best_match': None
            }
        
        # è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
        combined_similarity = (1 - feature_weight) * best_similarity + feature_weight * best_match['feature_similarity']
        
        # ç¡®å®šæ£€æµ‹ç±»å‹
        hash_distances = {
            'dhash': best_match['dhash_distance'],
            'phash': best_match['phash_distance'],
            'ahash': best_match['ahash_distance']
        }
        
        # æ„å»ºåŒ…å«æ—‹è½¬å’Œç¼©æ”¾ä¿¡æ¯çš„åŒ¹é…æ•°æ®ç”¨äºæ£€æµ‹ç±»å‹åˆ¤æ–­
        match_with_transform = {
            **best_match,
            'angle1': 0,
            'angle2': best_rotation,
            'scale1': 1.0,
            'scale2': best_scale,
            'size1': (100, 100),  # å ä½ç¬¦
            'size2': (100, 100)   # å ä½ç¬¦
        }
        detection_type = self._determine_detection_type(match_with_transform)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(match_with_transform, hash_distances)
        
        return {
            'is_similar': combined_similarity > 0.7,
            'confidence': confidence,
            'detection_type': detection_type,
            'best_match': {
                **best_match,
                'rotation_angle': best_rotation,
                'scale_factor': best_scale,
                'combined_similarity': combined_similarity
            }
        }


def batch_compare_with_enhanced_similarity(image_files: List[str],
                                         dhash_threshold: int = 12,
                                         phash_threshold: int = 4,
                                         ahash_threshold: int = 4,
                                         feature_weight: float = 0.3,
                                         confidence_threshold: float = 0.6,
                                         log_callback=None) -> List[Dict]:
    """
    æ‰¹é‡æ¯”è¾ƒå›¾ç‰‡ï¼Œæ”¯æŒæ—‹è½¬å’Œåˆ†è¾¨ç‡å˜åŒ–æ£€æµ‹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        image_files: å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
        dhash_threshold: dHashé˜ˆå€¼
        phash_threshold: pHashé˜ˆå€¼
        ahash_threshold: aHashé˜ˆå€¼
        feature_weight: ç‰¹å¾æƒé‡
        confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
        
    Returns:
        ç›¸ä¼¼å›¾ç‰‡ç»„åˆ—è¡¨
    """
    import time
    start_time = time.time()
    
    detector = EnhancedSimilarityDetector()
    similar_groups = []
    processed = set()
    
    def log(message):
        if log_callback:
            log_callback(message)
        else:
            print(message)
    
    total_files = len(image_files)
    total_comparisons = (total_files * (total_files - 1)) // 2
    
    log(f"å¼€å§‹å¢å¼ºç›¸ä¼¼åº¦æ£€æµ‹ï¼Œå…± {total_files} ä¸ªæ–‡ä»¶ï¼Œé¢„è®¡ {total_comparisons} æ¬¡æ¯”è¾ƒ...")
    log("æ³¨æ„ï¼šå¢å¼ºæ£€æµ‹è®¡ç®—é‡è¾ƒå¤§ï¼Œæ¯æ¬¡æ¯”è¾ƒéœ€è¦å¤šä¸ªå°ºåº¦å’Œè§’åº¦ç»„åˆ")
    
    # é¢„å…ˆè®¡ç®—æ‰€æœ‰å›¾ç‰‡çš„å“ˆå¸Œå€¼ä»¥æé«˜æ•ˆç‡
    log("æ­£åœ¨é¢„è®¡ç®—æ‰€æœ‰å›¾ç‰‡çš„å¤šå°ºåº¦å“ˆå¸Œå€¼...")
    log(f"æ¯ä¸ªå›¾ç‰‡å°†è®¡ç®— {len(detector.angles)} ä¸ªè§’åº¦ Ã— {len(detector.scales)} ä¸ªç¼©æ”¾ Ã— {len(detector.hash_sizes)} ä¸ªå“ˆå¸Œå¤§å° = {len(detector.angles) * len(detector.scales) * len(detector.hash_sizes)} ä¸ªå“ˆå¸Œç»„åˆ")
    all_hashes = {}
    for i, file_path in enumerate(image_files):
        try:
            # æ›´é¢‘ç¹çš„è¿›åº¦æŠ¥å‘Š
            log(f"é¢„è®¡ç®—è¿›åº¦: {i+1}/{total_files} ({(i+1)/total_files*100:.1f}%) - å¤„ç†: {os.path.basename(file_path)}")
            all_hashes[file_path] = detector.calculate_multi_scale_hashes(file_path)
            log(f"  å®Œæˆï¼Œç”Ÿæˆäº† {len(all_hashes[file_path])} ä¸ªå“ˆå¸Œç»„åˆ")
        except Exception as e:
            log(f"é¢„è®¡ç®—å“ˆå¸Œå¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            all_hashes[file_path] = []
    
    log("é¢„è®¡ç®—å®Œæˆï¼Œå¼€å§‹æ¯”è¾ƒ...")
    
    comparison_count = 0
    for i, file1 in enumerate(image_files):
        if file1 in processed:
            continue
        
        # æŠ¥å‘Šå½“å‰å¤„ç†çš„ä¸»æ–‡ä»¶
        elapsed = time.time() - start_time
        log(f"å¤„ç†ä¸»æ–‡ä»¶ {i+1}/{total_files} ({(i+1)/total_files*100:.1f}%): {os.path.basename(file1)} - å·²ç”¨æ—¶ {elapsed:.1f}ç§’")
        
        similar_files = []
        detection_info = []
        
        for j, file2 in enumerate(image_files[i+1:], i+1):
            if file2 in processed:
                continue
            
            comparison_count += 1
            
            # æ›´é¢‘ç¹çš„æ¯”è¾ƒè¿›åº¦æŠ¥å‘Š - æ¯5æ¬¡æ¯”è¾ƒæˆ–é‡è¦æ¯”è¾ƒæ—¶æŠ¥å‘Š
            if comparison_count % 5 == 0 or j == len(image_files) - 1:
                progress = comparison_count / total_comparisons * 100
                elapsed = time.time() - start_time
                estimated_total = elapsed / (comparison_count / total_comparisons) if comparison_count > 0 else 0
                remaining = estimated_total - elapsed
                log(f"  æ¯”è¾ƒè¿›åº¦: {comparison_count}/{total_comparisons} ({progress:.1f}%) - å½“å‰å¯¹æ¯”: {os.path.basename(file1)} vs {os.path.basename(file2)} - é¢„è®¡å‰©ä½™ {remaining:.0f}ç§’")
            
            try:
                # ä½¿ç”¨é¢„è®¡ç®—çš„å“ˆå¸Œå€¼è¿›è¡Œå¿«é€Ÿæ¯”è¾ƒ
                result = detector._fast_compare_with_precomputed_hashes(
                    all_hashes.get(file1, []), all_hashes.get(file2, []),
                    file1, file2, dhash_threshold, phash_threshold, 
                    ahash_threshold, feature_weight
                )
                
                if result and result['is_similar'] and result['confidence'] >= confidence_threshold:
                    similar_files.append(file2)
                    detection_info.append({
                        'file': file2,
                        'detection_type': result['detection_type'],
                        'confidence': result['confidence'],
                        'best_match': result['best_match']
                    })
                    
                    # è¯¦ç»†çš„ç›¸ä¼¼å›¾ç‰‡å‘ç°æ—¥å¿—
                    rotation_info = ""
                    if result['best_match'] and 'rotation_angle' in result['best_match']:
                        rotation_angle = result['best_match']['rotation_angle']
                        if rotation_angle > 0:
                            rotation_info = f" (æ—‹è½¬è§’åº¦: {rotation_angle}Â°)"
                    
                    scale_info = ""
                    if result['best_match'] and 'scale_factor' in result['best_match']:
                        scale_factor = result['best_match']['scale_factor']
                        if abs(scale_factor - 1.0) > 0.1:
                            scale_info = f" (ç¼©æ”¾æ¯”ä¾‹: {scale_factor:.2f})"
                    
                    log(f"  âœ“ æ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡: {os.path.basename(file1)} <-> {os.path.basename(file2)}")
                    log(f"    æ£€æµ‹ç±»å‹: {result['detection_type']}, ç½®ä¿¡åº¦: {result['confidence']:.3f}{rotation_info}{scale_info}")
            
            except Exception as e:
                log(f"æ¯”è¾ƒæ–‡ä»¶æ—¶å‡ºé”™: {file1} vs {file2}, é”™è¯¯: {str(e)}")
                continue
        
        if similar_files:
            all_files = [file1] + similar_files
            
            # æ„å»ºæ£€æµ‹åŸå› 
            detection_types = [info['detection_type'] for info in detection_info]
            unique_types = list(set(detection_types))
            reason = "å¢å¼ºæ£€æµ‹(" + "+".join(unique_types) + ")"
            
            similar_group = {
                'reason': reason,
                'files': all_files,
                'detection_info': detection_info,
                'avg_confidence': sum(info['confidence'] for info in detection_info) / len(detection_info)
            }
            
            similar_groups.append(similar_group)
            
            # è¯¦ç»†çš„ç›¸ä¼¼ç»„å‘ç°æ—¥å¿—
            log(f"\nğŸ¯ å‘ç°ç›¸ä¼¼ç»„ #{len(similar_groups)}: {len(all_files)} ä¸ªæ–‡ä»¶")
            log(f"  ä¸»æ–‡ä»¶: {os.path.basename(file1)}")
            for info in detection_info:
                log(f"  ç›¸ä¼¼æ–‡ä»¶: {os.path.basename(info['file'])} (ç½®ä¿¡åº¦: {info['confidence']:.3f}, ç±»å‹: {info['detection_type']})")
            log(f"  å¹³å‡ç½®ä¿¡åº¦: {similar_group['avg_confidence']:.3f}")
            log(f"  æ£€æµ‹åŸå› : {reason}\n")
            
            # æ ‡è®°ä¸ºå·²å¤„ç†
            for file_path in all_files:
                processed.add(file_path)
        else:
            log(f"  æœªæ‰¾åˆ°ä¸ {os.path.basename(file1)} ç›¸ä¼¼çš„å›¾ç‰‡")
    
    elapsed = time.time() - start_time
    log(f"å¢å¼ºç›¸ä¼¼åº¦æ£€æµ‹å®Œæˆï¼æ€»ç”¨æ—¶ {elapsed:.1f}ç§’ï¼Œæ‰¾åˆ° {len(similar_groups)} ä¸ªç›¸ä¼¼ç»„")
    return similar_groups